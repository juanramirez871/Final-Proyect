{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers==4.44.2 soundfile==0.12.1 sentencepiece==0.2.0 numpy==1.26.4 torchaudio==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juandiego/Documents/study/keepcoding/Final Proyect/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import re\n",
    "from IPython.display import Audio as IPyAudio, display\n",
    "from transformers import SpeechT5ForTextToSpeech, SpeechT5Processor, SpeechT5HifiGan\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "model_inf = SpeechT5ForTextToSpeech.from_pretrained(\"./speecht5_tts_colombian_final\").to(DEVICE)\n",
    "processor_inf = SpeechT5Processor.from_pretrained(\"./speecht5_tts_colombian_final\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\").to(DEVICE)\n",
    "model_inf.eval()\n",
    "spk_emb_avg = torch.tensor(np.load(\"./speaker_embedding.npy\"), dtype=torch.float32).unsqueeze(0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_texto(texto):\n",
    "    texto = texto.replace(\"¿\", \"\").replace(\"¡\", \"\")\n",
    "    texto = texto.replace(\",\", \"\").replace(\".\", \"\")\n",
    "    texto = texto.replace(\":\", \"\").replace(\";\", \"\")\n",
    "    \n",
    "    return re.sub(r'\\s+', ' ', texto).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_en_frases(texto, max_tokens=50):\n",
    "\n",
    "    PATRONES_CORTE = [\n",
    "        r',\\s+',\n",
    "        r'\\s+(pero|sino|aunque|porque|cuando|donde|como|mientras|después|antes|entonces|ya que)\\s+',\n",
    "        r'\\s+(y|e|o|u)\\s+(?=\\w{4,})',\n",
    "    ]\n",
    "    \n",
    "    n_tokens = processor_inf(text=texto, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "    if n_tokens <= max_tokens:\n",
    "        return [texto]\n",
    "\n",
    "    for patron in PATRONES_CORTE:\n",
    "        segmentos = re.split(f'({patron})', texto, flags=re.IGNORECASE)\n",
    "        partes_candidatas = []\n",
    "        chunk = \"\"\n",
    "\n",
    "        for seg in segmentos:\n",
    "            chunk += seg\n",
    "            tok = processor_inf(text=chunk.strip(), return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "\n",
    "            if tok >= max_tokens:\n",
    "                anterior = chunk[:-(len(seg))].strip()\n",
    "                if anterior:\n",
    "                    partes_candidatas.append(anterior)\n",
    "                chunk = seg.strip()\n",
    "\n",
    "        if chunk.strip():\n",
    "            partes_candidatas.append(chunk.strip())\n",
    "        if all(processor_inf(text=p, return_tensors=\"pt\")[\"input_ids\"].shape[1] <= max_tokens for p in partes_candidatas) and len(partes_candidatas) > 1:\n",
    "            return partes_candidatas\n",
    "        \n",
    "    palabras = texto.split()\n",
    "    partes, chunk_actual = [], []\n",
    "    for palabra in palabras:\n",
    "        chunk_actual.append(palabra)\n",
    "\n",
    "        if processor_inf(text=\" \".join(chunk_actual), return_tensors=\"pt\")[\"input_ids\"].shape[1] >= max_tokens:\n",
    "            partes.append(\" \".join(chunk_actual))\n",
    "            chunk_actual = []\n",
    "\n",
    "    if chunk_actual:\n",
    "        partes.append(\" \".join(chunk_actual))\n",
    "        \n",
    "    return partes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recortar_silencio(audio, sr=16000, margen_ms=150):\n",
    "    umbral = max(audio.abs().max().item() * 0.05, 0.001)\n",
    "    margen = int(sr * margen_ms / 1000)\n",
    "    indices = (audio.abs() > umbral).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    if len(indices) == 0:\n",
    "        return audio\n",
    "\n",
    "    return audio[max(0, indices[0].item() - margen):min(len(audio), indices[-1].item() + margen)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_chunk(texto, threshold=0.5, minlenratio=0.1):\n",
    "    inputs = processor_inf(text=texto.strip() + \" .\", return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        speech = model_inf.generate_speech(\n",
    "            inputs[\"input_ids\"],\n",
    "            speaker_embeddings=spk_emb_avg,\n",
    "            vocoder=vocoder,\n",
    "            threshold=threshold,\n",
    "            minlenratio=minlenratio,\n",
    "            maxlenratio=20.0,\n",
    "        )\n",
    "\n",
    "    return speech.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intentar_generar(parte, amp_minima=0.05, profundidad=0):\n",
    "    for params in [{\"threshold\": 0.5, \"minlenratio\": 0.1}, {\"threshold\": 0.4, \"minlenratio\": 0.2}, {\"threshold\": 0.6, \"minlenratio\": 0.0}]:\n",
    "        speech = generar_chunk(parte, params[\"threshold\"], params[\"minlenratio\"])\n",
    "        if speech.abs().max().item() >= amp_minima:\n",
    "            return [recortar_silencio(speech)]\n",
    "\n",
    "    palabras = parte.strip().split()\n",
    "    if len(palabras) <= 2 or profundidad >= 3:\n",
    "        return []\n",
    "\n",
    "    mitad = len(palabras) // 2\n",
    "    return (\n",
    "        intentar_generar(\" \".join(palabras[:mitad]), amp_minima, profundidad + 1)\n",
    "        + [torch.zeros(1600)]\n",
    "        + intentar_generar(\" \".join(palabras[mitad:]), amp_minima, profundidad + 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajustar_velocidad(audio: torch.Tensor, sr: int = 16000, velocidad: float = 0.85) -> torch.Tensor:\n",
    "    import torchaudio.functional as F\n",
    "    sr_nuevo = int(sr * velocidad)\n",
    "    audio_lento = F.resample(audio, sr_nuevo, sr)\n",
    "    \n",
    "    return audio_lento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_audio(texto, nombre=\"output.wav\", max_tokens=50):\n",
    "    \n",
    "    texto_limpio = preparar_texto(texto)\n",
    "    partes = dividir_en_frases(texto_limpio, max_tokens)\n",
    "    partes_filtradas = []\n",
    "    \n",
    "    for parte in partes:\n",
    "        n_tok = processor_inf(text=parte, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "        if n_tok < 15 and partes_filtradas:\n",
    "            partes_filtradas[-1] += \" \" + parte\n",
    "        else:\n",
    "            partes_filtradas.append(parte)\n",
    "    partes = partes_filtradas\n",
    "\n",
    "    audios = []\n",
    "    for j, parte in enumerate(partes):\n",
    "        audios += intentar_generar(parte)\n",
    "        if j < len(partes) - 1:\n",
    "            audios.append(torch.zeros(3200))\n",
    "\n",
    "    if not audios:\n",
    "        print(\"Error audio\")\n",
    "        return\n",
    "\n",
    "    audio_final = torch.cat(audios)\n",
    "    audio_final = ajustar_velocidad(audio_final, sr=16000, velocidad=0.96)\n",
    "    if audio_final.abs().max().item() > 0.001:\n",
    "        sf.write(nombre, audio_final.numpy(), samplerate=16000)\n",
    "        display(IPyAudio(nombre))\n",
    "    else:\n",
    "        print(\"Audio silencioso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generar_audio(\"Keep Coding es el mejor bootcamp de inteligencia artificial\", \"result.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
