{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pip_install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip install \\\n",
    "  transformers==4.49.0 \\\n",
    "  accelerate==1.4.0 \\\n",
    "  bitsandbytes==0.45.3 \\\n",
    "  peft==0.14.0 \\\n",
    "  trl==0.15.2 \\\n",
    "  datasets==3.3.2 \\\n",
    "  huggingface_hub==0.28.1\n",
    "  \n",
    "!pip install \\\n",
    "  \"numpy<2\" \\\n",
    "  pandas==2.2.3 \\\n",
    "  matplotlib==3.9.4 \\\n",
    "  seaborn==0.13.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify_md",
   "metadata": {},
   "source": [
    "Verifico el entorno en el que me encuentro ya que estoy en un jupyter remoto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "cap = torch.cuda.get_device_capability()\n",
    "_ = torch.tensor([1.0]).cuda() * 2\n",
    "import bitsandbytes as bnb\n",
    "import transformers, peft, trl, datasets\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Compute: sm_{cap[0]}{cap[1]}\")\n",
    "print(f\"VRAM total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45cbead",
   "metadata": {},
   "source": [
    "Hago login con huggingface para poder acceder a los modelos y datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hf_login",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_md",
   "metadata": {},
   "source": [
    "Descargo el dataset y hago un exploratorio y análisis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_dataset_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"JulianVelandia/ColombianAccent\", split=\"train\")\n",
    "\n",
    "print(f\"Total ejemplos: {len(raw_dataset)}\")\n",
    "print(f\"Columnas: {raw_dataset.column_names}\")\n",
    "\n",
    "print(\"Ejemplo:\")\n",
    "print(json.dumps(raw_dataset[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_lens = [len(ex[\"instruction\"].split()) for ex in raw_dataset]\n",
    "response_lens = [len(ex[\"response\"].split()) for ex in raw_dataset]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].hist(instruct_lens, bins=40, color='steelblue')\n",
    "axes[0].set_title('Longitud instrucciones (palabras)')\n",
    "axes[1].hist(response_lens, bins=40, color='coral')\n",
    "axes[1].set_title('Longitud respuestas (palabras)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4754b7",
   "metadata": {},
   "source": [
    "Hago un preprocesamiento del dataset para adaptarlo al formato que necesito para el fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"Eres un asesor comercial colombiano parce, trabajas vendiendo productos y servicios en Colombia. \"\n",
    "    \"Hablas con el acento, las palabras y las expresiones típicas colombianas \"\n",
    "    \"'chimba', 'hagale', 'de una', 'listo pues', 'sumerce', 'a lo bien', entre otras. \"\n",
    "    \"Tu objetivo es atender al cliente de manera cálida y cercana, resolver sus dudas, manejar objeciones \"\n",
    "    \"con argumentos sólidos, y cerrar la venta de forma natural. Eres persuasivo pero honesto, nunca presionas \"\n",
    "    \"de mala manera. Siempre mantienes el tono 100% colombiano en cada respuesta.\"\n",
    ")\n",
    "\n",
    "def format_example(example):\n",
    "    text = (\n",
    "        \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "        f\"{SYSTEM_PROMPT}<|eot_id|>\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"{example['instruction']}<|eot_id|>\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        f\"{example['response']}<|eot_id|>\"\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "formatted_dataset = raw_dataset.map(\n",
    "    format_example,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {len(formatted_dataset)} ejemplos\")\n",
    "print(\"Ejemplo:\")\n",
    "print(formatted_dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdde24c",
   "metadata": {},
   "source": [
    "Divido el dataset en entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = formatted_dataset.train_test_split(test_size=0.05, seed=SEED)\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": split[\"train\"],\n",
    "    \"validation\": split[\"test\"]\n",
    "})\n",
    "\n",
    "print(f\"Train: {len(dataset_dict['train'])} ejemplos\")\n",
    "print(f\"Validation: {len(dataset_dict['validation'])} ejemplos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_md",
   "metadata": {},
   "source": [
    "Cargo el modelo base que se va a fine-tuning, y configuro el tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lora_md",
   "metadata": {},
   "source": [
    "Preparo la configuración de LoRA para el fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lora_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_md",
   "metadata": {},
   "source": [
    "Preparo la configuración de entrenamiento para el SFTTrainer, 2 épocas y un learning rate de 1e-4 porque cuento con 30k ejemplos en el dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_args",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"/llama_ventas_co_checkpoints\"\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.3,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a109b",
   "metadata": {},
   "source": [
    "Inicio el entrenamiento del modelo con el lora, tokenizador, training_args y el dataset ya preprocesado, tambien aplico un early stopping para evitar seguir entrenando si el modelo no mejora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.005)],\n",
    ")\n",
    "\n",
    "trainer.create_model_card = lambda *args, **kwargs: None\n",
    "trainable = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in trainer.model.parameters())\n",
    "\n",
    "print(f\"Parámetros entrenables: {trainable:,} ({100 * trainable / total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics_md",
   "metadata": {},
   "source": [
    "Analizo las métricas de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history = trainer.state.log_history\n",
    "\n",
    "train_logs = [log for log in log_history if 'loss' in log and 'eval_loss' not in log]\n",
    "eval_logs  = [log for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "if train_logs:\n",
    "    axes[0].plot([l['step'] for l in train_logs], [l['loss'] for l in train_logs],\n",
    "                 linewidth=2, color='steelblue')\n",
    "    axes[0].set_xlabel('Steps'); axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training Loss', fontweight='bold'); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "if eval_logs:\n",
    "    axes[1].plot([l['epoch'] for l in eval_logs], [l['eval_loss'] for l in eval_logs],\n",
    "                 marker='s', linewidth=2, color='coral')\n",
    "    axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Eval Loss')\n",
    "    axes[1].set_title('Validation Loss', fontweight='bold'); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if eval_logs:\n",
    "    print(f\"Train loss final: {train_logs[-1]['loss']:.4f}\")\n",
    "    print(f\"Val loss final:   {eval_logs[-1]['eval_loss']:.4f}\")\n",
    "    print(f\"Mejor val loss:   {min(l['eval_loss'] for l in eval_logs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_md",
   "metadata": {},
   "source": [
    "Pruebas finales con el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_message: str, max_new_tokens: int = 60) -> str:\n",
    "    prompt = (\n",
    "        \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "        f\"{SYSTEM_PROMPT}<|eot_id|>\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"{user_message}<|eot_id|>\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    eos_token_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.3,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=[\n",
    "                    tokenizer.eos_token_id,\n",
    "                    eos_token_id,\n",
    "                ],\n",
    "            )\n",
    "\n",
    "    return tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "test_prompts = [\n",
    "    \"Hola, buenos días\",\n",
    "    \"Ese precio me parece muy caro\",\n",
    "    \"¿Qué garantía tiene el producto?\",\n",
    "    \"Déjeme pensarlo y le aviso\",\n",
    "    \"¿Por qué debería comprarle a usted y no a la competencia?\",\n",
    "    \"No tengo plata ahorita\",\n",
    "    \"¿Hacen descuentos?\",\n",
    "    \"Muchas gracias, me interesa\",\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{i} Cliente: {prompt}\")\n",
    "    print(f\"Asesor: {generate_response(prompt)}\")\n",
    "    print(\"\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_md",
   "metadata": {},
   "source": [
    "Guardo el modelo entrenado y el tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MODEL_PATH = \"/llama_ventas_colombiano_LoRA\"\n",
    "\n",
    "trainer.model.save_pretrained(FINAL_MODEL_PATH)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
